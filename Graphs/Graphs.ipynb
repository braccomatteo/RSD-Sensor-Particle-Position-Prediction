{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_dev=pd.read_csv('C:\\\\Users\\\\picca\\\\Documents\\\\DataScienceLab\\\\DataScience\\\\January_project\\\\dataset_wp\\\\DSL_Winter_Project_2024\\\\development.csv',sep= ',')\n",
    "df_eval = pd.read_csv('C:\\\\Users\\\\picca\\\\Documents\\\\DataScienceLab\\\\DataScience\\\\January_project\\\\dataset_wp\\\\DSL_Winter_Project_2024\\\\evaluation.csv',sep= ',')\n",
    "\n",
    "df_dev_coord = df_dev[['x', 'y']]\n",
    "df_dev_train = df_dev.drop(columns=['x', 'y'])\n",
    "fig, axs = plt.subplots(nrows=3, ncols=6, figsize=(24, 16))\n",
    "\n",
    "def objective(x, mu, sigma, a, b, c) :\n",
    "   return a*1/(np.sqrt(2*np.pi)*sigma*b*x)*np.exp(-((np.log(b*x)- mu)**2)/(2*sigma**2)) + c\n",
    "\n",
    "listmafev = [280, 280, 280, 300,280, 300,280, 280,280, 280, 280, 300]\n",
    "k=0\n",
    "l=0\n",
    "j=0\n",
    "for i in [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 13, 14]:\n",
    "    print(i , \"\\n\")\n",
    "    df_dev_train_i = (df_dev[[f\"pmax[{i}]\"]])\n",
    "    histog = axs[j % 3, l % 6].hist(df_dev_train_i, 300)\n",
    "    print(listmafev[k])\n",
    "    print(k)\n",
    "    params, covariance = curve_fit(objective, list(histog[1])[:300], list(histog[0]), maxfev= listmafev[k])\n",
    "    print(params)\n",
    "    mu, sigma, a, b, c = params\n",
    "    x = np.linspace(histog[1].min(), histog[1].max(), 300)\n",
    "    y = objective(x,  mu, sigma, a, b, c)\n",
    "    axs[j % 3, l % 6].plot(x, y, color='red', linestyle='solid', linewidth=2, label='Fit (Lognormal)')\n",
    "    axs[j % 3, l % 6].set_title(f'pmax[{i}]')\n",
    "    axs[j % 3, l % 6].legend(fontsize='medium')\n",
    "    k += 1\n",
    "    l += 1\n",
    "    if(l % 6 == 0):\n",
    "        j += 1\n",
    "\n",
    "def normal(x, mu, sigma, a):\n",
    "    return a*np.exp(-(x-mu)**2/(2*sigma**2))\n",
    "\n",
    "listmafev = [ 280,280, 280]\n",
    "k=0\n",
    "l=0\n",
    "for i in [0, 7, 12]:\n",
    "    print(i , \"\\n\")\n",
    "    df_dev_train_i = df_dev[[f\"pmax[{i}]\"]]\n",
    "    q999 = df_dev_train_i[f\"pmax[{i}]\"].quantile(.980)\n",
    "    q001 = df_dev_train_i[f\"pmax[{i}]\"].quantile(.001)\n",
    "    print(q999, q001)\n",
    "    mask = (df_dev_train_i[f\"pmax[{i}]\"] < q999) & (df_dev_train_i[f\"pmax[{i}]\"] > q001)\n",
    "    df_dev_train_i = df_dev_train_i.loc[mask]\n",
    "    print(df_dev_train_i.shape)\n",
    "    histog = axs[2, l].hist(df_dev_train_i, 200)\n",
    "    print(listmafev[k])\n",
    "    print(k)\n",
    "    params, covariance = curve_fit(normal, list(histog[1])[:200], list(histog[0]), maxfev= listmafev[k])\n",
    "    print(params)\n",
    "    mu, sigma, a = params\n",
    "    x = np.linspace(df_dev_train_i.min(), df_dev_train_i.max(), 200)\n",
    "    y = normal(x, mu, sigma, a)\n",
    "    axs[2, l].plot(x, y, color='red', linestyle='solid', linewidth=2, label='Fit (Normal)')\n",
    "    axs[2, l].set_title(f'pmax[{i}]')\n",
    "    axs[2, l].legend(fontsize='medium')\n",
    "    l += 1\n",
    "    k += 1\n",
    "\n",
    "\n",
    "k=0\n",
    "l=3\n",
    "for i in [ 15, 16, 17]:\n",
    "    print(i , \"\\n\")\n",
    "    df_dev_train_i = df_dev[[f\"pmax[{i}]\"]]\n",
    "    q999 = df_dev_train_i[f\"pmax[{i}]\"].quantile(.995)\n",
    "    q001 = df_dev_train_i[f\"pmax[{i}]\"].quantile(.005)\n",
    "    print(q999, q001)\n",
    "    mask = (df_dev_train_i[f\"pmax[{i}]\"] < q999) & (df_dev_train_i[f\"pmax[{i}]\"] > q001)\n",
    "    df_dev_train_i = df_dev_train_i.loc[mask]\n",
    "    print(df_dev_train_i.shape, df_dev_train_i.mean().shape, df_dev_train_i.std().shape)\n",
    "\n",
    "    histog = axs[2, l].hist(df_dev_train_i, 100)\n",
    "    axs[2, l].set_title(f'pmax[{i}]')\n",
    "    l +=1\n",
    "plt.savefig(\"pmax_distributions.png\", dpi=300)  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import r2_score,mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "features_eliminated = []\n",
    "\n",
    "df_dev_drop_train = df_dev_train.drop(columns = features_eliminated)\n",
    "\n",
    "fetures_names = df_dev_drop_train.columns\n",
    "X = df_dev_drop_train.values\n",
    "X.shape\n",
    "y = df_dev_coord.values\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, shuffle=True, random_state=42, test_size = 0.20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating list_euclideans_rf and list_depths_rf for max_depth hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "import matplotlib.pyplot as plt\n",
    "params = {\n",
    "    \"max_depth\": [10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40],\n",
    "    \"n_estimators\": [100],\n",
    "    \"random_state\": [42],\n",
    "    \"max_features\": [\"sqrt\"],\n",
    "    \"criterion\": [\"friedman_mse\"]\n",
    "}\n",
    "list_euclideans_rf = []\n",
    "list_depths_rf  = []\n",
    "for config in ParameterGrid(params):\n",
    "    print(\"\\n\" + \"\\n\")\n",
    "    regr_multi = MultiOutputRegressor(\n",
    "    RandomForestRegressor(**config)\n",
    "    )\n",
    "    regr_multi.fit(X_train, y_train)\n",
    "    y_pred = regr_multi.predict(X_valid)\n",
    "    eucl_dist = np.sum(np.sum(((y_valid - y_pred) )**(2), axis=1)**(1/2), axis=0) * 1/(y_valid.size / 2)\n",
    "    list_euclideans_rf = list_euclideans_rf + [eucl_dist]\n",
    "    list_depths_rf = list_depths_rf + [config.get(\"n_estimators\")]\n",
    "    print(config, \"\\t ---> \" ,eucl_dist)\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": [16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40],\n",
    "    \"n_estimators\": [100],\n",
    "    \"random_state\": [42],\n",
    "    \"max_features\": [\"sqrt\"],\n",
    "    \"criterion\": [\"friedman_mse\"]\n",
    "}\n",
    "list_euclideans_et = []\n",
    "list_depths_et = []\n",
    "for config in ParameterGrid(params):\n",
    "    print(\"\\n\" + \"\\n\")\n",
    "    regr_multi = MultiOutputRegressor(\n",
    "    ExtraTreesRegressor(**config)\n",
    "    )\n",
    "    regr_multi.fit(X_train, y_train)\n",
    "    y_pred = regr_multi.predict(X_valid)\n",
    "    eucl_dist = np.sum(np.sum(((y_valid - y_pred) )**(2), axis=1)**(1/2), axis=0) * 1/(y_valid.size / 2)\n",
    "    list_euclideans_et = list_euclideans_et + [eucl_dist]\n",
    "    list_depths_et = list_depths_et + [config.get(\"n_estimators\")]\n",
    "    print(config, \"\\t ---> \" ,eucl_dist)\n",
    "\n",
    "list_euclideans_dt = []\n",
    "list_depths_dt = []\n",
    "for config in ParameterGrid(params):\n",
    "    print(\"\\n\" + \"\\n\")\n",
    "    regr_multi = MultiOutputRegressor(\n",
    "    DecisionTreeRegressor(**config)\n",
    "    )\n",
    "    regr_multi.fit(X_train, y_train)\n",
    "    y_pred = regr_multi.predict(X_valid)\n",
    "    eucl_dist = np.sum(np.sum(((y_valid - y_pred) )**(2), axis=1)**(1/2), axis=0) * 1/(y_valid.size / 2)\n",
    "    list_euclideans_dt = list_euclideans_dt + [eucl_dist]\n",
    "    list_depths_dt = list_depths_dt + [config.get(\"max_depth\")]\n",
    "    print(config, \"\\t ---> \" ,eucl_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "list_depths = [10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40]\n",
    "list_depths_et = [16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40]\n",
    "ax.plot(list_depths, list_euclideans_rf, c='red', linestyle =\"--\", label= \"random forest\", linewidth = 3)\n",
    "ax.plot(list_depths, list_euclideans_dt, c='blue', label=\"decision tree\",  linewidth = 3)\n",
    "ax.plot(list_depths_et, list_euclideans_et, c='green', linestyle='dotted', label=\"extra trees\", linewidth = 3)\n",
    "ax.legend(loc=\"upper right\",  fontsize=\"20\" )\n",
    "plt.xlabel(\"Max depth\")\n",
    "plt.ylabel(\"Avg. Eucl. ditance\")\n",
    "plt.savefig(\"tuning_maxdepth.pdf\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating list_euclideans_rf and list_depths_rf for n_estimators hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "import matplotlib.pyplot as plt\n",
    "params = {\n",
    "    \"max_depth\": [18],\n",
    "    \"n_estimators\": [50, 100, 150, 200,  250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800],\n",
    "    \"random_state\": [42],\n",
    "    \"max_features\": [\"sqrt\"],\n",
    "    \"criterion\": [\"friedman_mse\"]\n",
    "}\n",
    "list_euclideans_rf = []\n",
    "list_depths_rf  = []\n",
    "for config in ParameterGrid(params):\n",
    "    print(\"\\n\" + \"\\n\")\n",
    "    regr_multi = MultiOutputRegressor(\n",
    "    RandomForestRegressor(**config)\n",
    "    )\n",
    "    regr_multi.fit(X_train, y_train)\n",
    "    y_pred = regr_multi.predict(X_valid)\n",
    "    eucl_dist = np.sum(np.sum(((y_valid - y_pred) )**(2), axis=1)**(1/2), axis=0) * 1/(y_valid.size / 2)\n",
    "    list_euclideans_rf = list_euclideans_rf + [eucl_dist]\n",
    "    list_depths_rf = list_depths_rf + [config.get(\"n_estimators\")]\n",
    "    print(config, \"\\t ---> \" ,eucl_dist)\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": [18],\n",
    "    \"n_estimators\": [50, 100, 150, 200,  250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800],\n",
    "    \"random_state\": [42],\n",
    "    \"max_features\": [\"sqrt\"],\n",
    "    \"criterion\": [\"friedman_mse\"]\n",
    "}\n",
    "list_euclideans_et = []\n",
    "list_depths_et = []\n",
    "for config in ParameterGrid(params):\n",
    "    print(\"\\n\" + \"\\n\")\n",
    "    regr_multi = MultiOutputRegressor(\n",
    "    ExtraTreesRegressor(**config)\n",
    "    )\n",
    "    regr_multi.fit(X_train, y_train)\n",
    "    y_pred = regr_multi.predict(X_valid)\n",
    "    eucl_dist = np.sum(np.sum(((y_valid - y_pred) )**(2), axis=1)**(1/2), axis=0) * 1/(y_valid.size / 2)\n",
    "    list_euclideans_et = list_euclideans_et + [eucl_dist]\n",
    "    list_depths_et = list_depths_et + [config.get(\"n_estimators\")]\n",
    "    print(config, \"\\t ---> \" ,eucl_dist)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "list_nestimators = [50, 100, 150, 200,  250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800]\n",
    "ax.plot(list_nestimators, list_euclideans_rf, c='red', linestyle =\"--\", label= \"random forest\", linewidth = 3)\n",
    "ax.plot(list_nestimators, list_euclideans_dt, c='green', linestyle =\"dotted\", label=\"extra trees\",  linewidth = 3)\n",
    "ax.legend(loc=\"upper right\",  fontsize=\"20\" )\n",
    "plt.xlabel(\"N. Estimators\")\n",
    "plt.ylabel(\"Avg. Eucl. ditance\")\n",
    "plt.savefig(\"tuning_nestimators.pdf\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
